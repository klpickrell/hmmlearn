# Dask Hidden Markov Models

"""
The :mod:`hmmlearn.dask_hmm` module implements hidden Markov models with Dask support.
"""

import numpy as np
from scipy.special import logsumexp
from sklearn import cluster
from sklearn.utils import check_array, check_random_state
from sklearn.utils.validation import check_is_fitted

from .stats import log_multivariate_normal_density
from .base import _BaseHMM, ConvergenceMonitor
from .utils import iter_from_X_lengths, normalize, fill_covars, distribute_covar_matrix_to_match_covariance_type, _validate_covars

__all__ = ["DaskMultinomialHMM"]

COVARIANCE_TYPES = frozenset(("spherical", "diag", "full", "tied"))
DECODER_ALGORITHMS = frozenset(("viterbi", "map"))

class DaskMultinomialHMM(_BaseHMM):
    """Hidden Markov Model with multinomial (discrete) emissions (Dask)

    Parameters
    ----------

    n_components : int
        Number of states.

    n_features : int
        Number of symbols expected.

    startprob_prior : array, shape (n_components, ), optional
        Parameters of the Dirichlet prior distribution for
        :attr:`startprob_`.

    transmat_prior : array, shape (n_components, n_components), optional
        Parameters of the Dirichlet prior distribution for each row
        of the transition probabilities :attr:`transmat_`.

    algorithm : string, optional
        Decoder algorithm. Must be one of "viterbi" or "map".
        Defaults to "viterbi".

    random_state: RandomState or an int seed, optional
        A random number generator instance.

    n_iter : int, optional
        Maximum number of iterations to perform.

    tol : float, optional
        Convergence threshold. EM will stop if the gain in log-likelihood
        is below this value.

    verbose : bool, optional
        When ``True`` per-iteration convergence reports are printed
        to :data:`sys.stderr`. You can diagnose convergence via the
        :attr:`monitor_` attribute.

    params : string, optional
        Controls which parameters are updated in the training
        process.  Can contain any combination of 's' for startprob,
        't' for transmat, 'e' for emissionprob.
        Defaults to all parameters.

    init_params : string, optional
        Controls which parameters are initialized prior to
        training.  Can contain any combination of 's' for
        startprob, 't' for transmat, 'e' for emissionprob.
        Defaults to all parameters.

    Attributes
    ----------
    n_features : int
        Number of possible symbols emitted by the model (in the samples).

    monitor\_ : ConvergenceMonitor
        Monitor object used to check the convergence of EM.

    transmat\_ : array, shape (n_components, n_components)
        Matrix of transition probabilities between states.

    startprob\_ : array, shape (n_components, )
        Initial state occupation distribution.

    emissionprob\_ : array, shape (n_components, n_features)
        Probability of emitting a given symbol when in each state.

    Examples
    --------
    >>> from hmmlearn.hmm import DaskMultinomialHMM
    >>> DaskMultinomialHMM(n_components=2)
    ...                             #doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
    DaskMultinomialHMM(algorithm='viterbi',...
    """
    # TODO: accept the prior on emissionprob_ for consistency.
    def __init__(self, n_features, n_components=1,
                 startprob_prior=1.0, transmat_prior=1.0,
                 algorithm="viterbi", random_state=None,
                 n_iter=10, tol=1e-2, verbose=False,
                 params="ste", init_params="ste"):
        _BaseHMM.__init__(self, n_components,
                          startprob_prior=startprob_prior,
                          transmat_prior=transmat_prior,
                          algorithm=algorithm,
                          random_state=random_state,
                          n_iter=n_iter, tol=tol, verbose=verbose,
                          params=params, init_params=init_params)
        self.n_features = n_features

    def _init(self, X):
        """Initializes model parameters prior to fitting.

        Parameters
        ----------
        X : Dask dataframe
        """

# unavailable due to compute
#        if not self._check_input_symbols(X):
#            raise ValueError("expected a sample from "
#                             "a multinomial distribution.")

        init = 1. / self.n_components
        if 's' in self.init_params or not hasattr(self, "startprob_"):
            self.startprob_ = np.full(self.n_components, init)
        if 't' in self.init_params or not hasattr(self, "transmat_"):
            self.transmat_ = np.full((self.n_components, self.n_components),
                                     init)
        self.random_state = check_random_state(self.random_state)

        if 'e' in self.init_params:
            self.emissionprob_ = self.random_state \
                .rand(self.n_components, self.n_features)
            normalize(self.emissionprob_, axis=1)

    def _check(self):
        """Validates model parameters prior to fitting.

        Raises
        ------

        ValueError
            If any of the parameters are invalid, e.g. if :attr:`startprob_`
            don't sum to 1.
        """
        self.startprob_ = np.asarray(self.startprob_)
        if len(self.startprob_) != self.n_components:
            raise ValueError("startprob_ must have length n_components")
        if not np.allclose(self.startprob_.sum(), 1.0):
            raise ValueError("startprob_ must sum to 1.0 (got {0:.4f})"
                             .format(self.startprob_.sum()))

        self.transmat_ = np.asarray(self.transmat_)
        if self.transmat_.shape != (self.n_components, self.n_components):
            raise ValueError(
                "transmat_ must have shape (n_components, n_components)")
        if not np.allclose(self.transmat_.sum(axis=1), 1.0):
            raise ValueError("rows of transmat_ must sum to 1.0 (got {0})"
                             .format(self.transmat_.sum(axis=1)))

        self.emissionprob_ = np.atleast_2d(self.emissionprob_)
        n_features = getattr(self, "n_features", self.emissionprob_.shape[1])
        if self.emissionprob_.shape != (self.n_components, n_features):
            raise ValueError(
                "emissionprob_ must have shape (n_components, n_features)")
        else:
            self.n_features = n_features

    def fit(self, X):
        """Estimate model parameters.

        An initialization step is performed before entering the
        EM algorithm. If you want to avoid this step for a subset of
        the parameters, pass proper ``init_params`` keyword argument
        to estimator's constructor.

        Parameters
        ----------
        X : Dask dataframe

        Returns
        -------
        self : object
            Returns self.
        """
        self._init(X)
        self._check()

        self.monitor_ = ConvergenceMonitor(self.tol, self.n_iter, self.verbose)
        for iter in range(self.n_iter):
            # distribute chunks
            def process( X ):
                stats = self._initialize_sufficient_statistics()
                curr_logprob = 0
                for idx,symbols in X.iterrows():
                    j = symbols.values[0]
                    framelogprob = self._compute_log_likelihood(j)
                    logprob, fwdlattice = self._do_forward_pass(framelogprob)
                    curr_logprob += logprob
                    bwdlattice = self._do_backward_pass(framelogprob)
                    posteriors = self._compute_posteriors(fwdlattice, bwdlattice)
                    self._accumulate_sufficient_statistics(stats, j, framelogprob, posteriors, fwdlattice,bwdlattice)
                stats['logprob'] += curr_logprob
                return stats
            result = X.map_partitions(process, meta={'nobs':'i8','start':'f8','obs':'f8','trans':'f8','logprob':'f8'})
            aggstats = result.compute()
            stats = self._initialize_sufficient_statistics()
            for s in aggstats:
                stats['nobs'] += s['nobs']
                stats['start'] += s['start']
                stats['obs'] += s['obs']
                stats['trans'] += s['trans']
                stats['logprob'] += s['logprob']
            # end distribute chunks

            # XXX must be before convergence check, because otherwise
            #     there won't be any updates for the case ``n_iter=1``.
            self._do_mstep(stats)

            self.monitor_.report(stats['logprob'])
            if self.monitor_.converged:
                break

        return self

    def _compute_log_likelihood(self, X):
#        return np.log(self.emissionprob_)[:, np.concatenate(X)].T
        return np.log(self.emissionprob_)[:,X].T

    def _generate_sample_from_state(self, state, random_state=None):
        cdf = np.cumsum(self.emissionprob_[state, :])
        random_state = check_random_state(random_state)
        return [(cdf > random_state.rand()).argmax()]

    def _initialize_sufficient_statistics(self):
        stats = {'nobs': 0,
                 'start': np.zeros(self.n_components),
                 'trans': np.zeros((self.n_components, self.n_components)),
                 'obs': np.zeros((self.n_components, self.n_features)),
                 'logprob' : 0}
        return stats

    def _accumulate_sufficient_statistics(self, stats, X, framelogprob,
                                          posteriors, fwdlattice, bwdlattice):

        super(DaskMultinomialHMM, self)._accumulate_sufficient_statistics(
            stats, X, framelogprob, posteriors, fwdlattice, bwdlattice)
        if 'e' in self.params:
            for t, symbol in enumerate(X):
                stats['obs'][:, symbol] += posteriors[t]


    def _do_mstep(self, stats):
        """Performs the M-step of EM algorithm.

        Parameters
        ----------
        stats : dict
            Sufficient statistics updated from all available samples.
        """
        # The ``np.where`` calls guard against updating forbidden states
        # or transitions in e.g. a left-right HMM.
        if 's' in self.params:
            startprob_ = self.startprob_prior - 1.0 + stats['start']
            self.startprob_ = np.where(self.startprob_ == 0.0,
                                       self.startprob_, startprob_)
            normalize(self.startprob_)
        if 't' in self.params:
            transmat_ = self.transmat_prior - 1.0 + stats['trans']
            self.transmat_ = np.where(self.transmat_ == 0.0,
                                      self.transmat_, transmat_)
            normalize(self.transmat_, axis=1)
        if 'e' in self.params:
            self.emissionprob_ = (stats['obs']
                                  / stats['obs'].sum(axis=1)[:, np.newaxis])

    # this is borked for parellel, but might be useful if correct...
    def _check_input_symbols(self, X):
        """Check if ``X`` is a sample from a multinomial distribution.

        That is ``X`` should be an array of non-negative integers from
        range ``[min(X), max(X)]``, such that each integer from the range
        occurs in ``X`` at least once.

        For example ``[0, 0, 2, 1, 3, 1, 1]`` is a valid sample from a
        Multinomial distribution, while ``[0, 0, 3, 5, 10]`` is not.
        """
        symbols = np.concatenate(X)
        if (len(symbols) == 1 or          # not enough data
            symbols.dtype.kind != 'i' or  # not an integer
            (symbols < 0).any()):         # contains negative integers
            return False

        symbols.sort()
        return np.all(np.diff(symbols) <= 1)

    def score_samples(self, X):
        """Compute the log probability under the model and compute posteriors.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Feature matrix of individual samples.

        Returns
        -------
        logprob : float
            Log likelihood of ``X``.

        posteriors : array, shape (n_samples, n_components)
            State-membership probabilities for each sample in ``X``.

        See Also
        --------
        score : Compute the log probability under the model.
        decode : Find most likely state sequence corresponding to ``X``.
        """
        check_is_fitted(self, "startprob_")
        self._check()

#        posteriors[i:j] = self._compute_posteriors(fwdlattice, bwdlattice)

        def process(X):
#            X = check_array(X)
            n_samples = X.shape[0]
            posteriors = np.zeros((n_samples, self.n_components))
            result = { 'logprob' : 0, 'posteriors' : [] }
            for idx,symbols in X.iterrows():
                j = symbols.values[0]
                framelogprob = self._compute_log_likelihood(j)
                logprob, fwdlattice = self._do_forward_pass(framelogprob)
                result['logprob'] += logprob
                bwdlattice = self._do_backward_pass(framelogprob)
                posteriors = self._compute_posteriors(fwdlattice, bwdlattice)
                result['posteriors'].append(posteriors)
            return result

        result = X.map_partitions(process, meta={'logprob':'f8','posteriors':object})
        result = result.compute()
        logprob = 0
        for r in result:
            logprob += r['logprob']
        posteriors = np.concatenate(result['posteriors'])
        return logprob, posteriors

    def score(self, X):
        """Compute the log probability under the model.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Feature matrix of individual samples.

        Returns
        -------
        logprob : float
            Log likelihood of ``X``.

        See Also
        --------
        score_samples : Compute the log probability under the model and
            posteriors.
        decode : Find most likely state sequence corresponding to ``X``.
        """
        check_is_fitted(self, "startprob_")
        self._check()

        def process(X):
#            X = check_array(X)
            n_samples = X.shape[0]
            result = { 'logprob' : 0 }
            for idx,symbols in X.iterrows():
                j = symbols.values[0]
                framelogprob = self._compute_log_likelihood(j)
                logprob, _fwdlattice = self._do_forward_pass(framelogprob)
                result['logprob'] += logprob
            return result

        result = X.map_partitions(process, meta={'logprob':'f8'})
        result = result.compute()
        logprob = 0
        for r in result:
            logprob += r['logprob']
        return logprob

    def _decode_viterbi(self, X):
        framelogprob = self._compute_log_likelihood(X)
        return self._do_viterbi_pass(framelogprob)

    def _decode_map(self, X):
        _, posteriors = self.score_samples(X)
        logprob = np.max(posteriors, axis=1).sum()
        state_sequence = np.argmax(posteriors, axis=1)
        return logprob, state_sequence

    def decode(self, X, algorithm=None):
        """Find most likely state sequence corresponding to ``X``.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Feature matrix of individual samples.

        algorithm : string
            Decoder algorithm. Must be one of "viterbi" or "map".
            If not given, :attr:`decoder` is used.

        Returns
        -------
        logprob : float
            Log probability of the produced state sequence.

        state_sequence : array, shape (n_samples, )
            Labels for each sample from ``X`` obtained via a given
            decoder ``algorithm``.

        See Also
        --------
        score_samples : Compute the log probability under the model and
            posteriors.
        score : Compute the log probability under the model.
        """
        check_is_fitted(self, "startprob_")
        self._check()

        algorithm = algorithm or self.algorithm
        if algorithm not in DECODER_ALGORITHMS:
            raise ValueError("Unknown decoder {0!r}".format(algorithm))

        decoder = {
            "viterbi": self._decode_viterbi,
            "map": self._decode_map
        }[algorithm]

        def process(X):
#        X = check_array(X)
            n_samples = X.shape[0]
            result = {'logprob' : 0, 'states' : [] }
            for idx,symbols in X.iterrows():
                j = symbols.values[0]
                logprob, states = decoder(j)
                result['logprob'] += logprob
                result['states'].append(states)
            return result

        if 'map_partitions' in X:
            result = X.map_partitions(process,meta={'logprob':'f8','states':object})
            result = result.compute()
        else:
            result = X.apply(process)

        states = []
        logprob = 0
        for r in result:
            logprob += r['logprob']
            states.append(r['states'])

        return logprob,np.concatenate(states)


    def predict(self, X):
        """Find most likely state sequence corresponding to ``X``.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Feature matrix of individual samples.

        Returns
        -------
        state_sequence : array, shape (n_samples, )
            Labels for each sample from ``X``.
        """
        _, state_sequence = self.decode(X)
        return state_sequence

    def predict_proba(self, X):
        """Compute the posterior probability for each state in the model.

        X : array-like, shape (n_samples, n_features)
            Feature matrix of individual samples.

        Returns
        -------
        posteriors : array, shape (n_samples, n_components)
            State-membership probabilities for each sample from ``X``.
        """
        _, posteriors = self.score_samples(X)
        return posteriors

